{
   "configurations" : [
	{ "global": {
		 "hive_metastore_user_passwd": "{{ HIVE_METASTORE_PASSWORD }}" 
	}
      },
     {
       "core-site": {
            "hadoop.proxyuser.hive.groups": "*",
            "hadoop.proxyuser.hive.hosts": "*",
            "hadoop.proxyuser.hcat.groups": "*",
            "hadoop.proxyuser.hcat.hosts": "*"
        }
    },
      {
      "hdfs-site" : {
        "dfs.replication" : "1",
        "dfs.datanode.data.dir": "{{DATANODE_DATA_DIR}}"
      }
    },
    {
       "hadoop-env" : {
           "hdfs_log_dir_prefix" : "{{ LOG_DIR }}/hadoop"
        }
     },
     {
       "yarn-site" : {
	   "yarn.timeline-service.store-class" : "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore",
           "yarn.nodemanager.log-dirs" : "{{ LOG_DIR }}/yarn/log"
        }
     },
     {
       "yarn-env" : {
           "yarn_log_dir_prefix" : "{{ LOG_DIR }}/yarn"
        }
     },
     {
       "zookeeper-env" : {
           "zk_log_dir" : "{{ LOG_DIR }}/zookeeper"
        }
      },
      {
      "mapred-env" : {
           "mapred_log_dir_prefix" : "{{ LOG_DIR }}/hadoop-mapreduce"
        }
      },
      {
      "spark-env" : {
 	   "spark_log_dir" : "{{ LOG_DIR }}/spark"		
        }	    
      },
     {
       "tez-site" : {
       "tez.am.client.am.port-range": "12996"
       }
     },
     {
       "hbase-site" : {
       "hbase.rootdir": "hdfs://{{hostsIP.stdout_lines[0]}}:8020/apps/hbase/data"
       }
     },
      {
      "hive-env" : {
           "hcat_log_dir" : "{{ LOG_DIR }}/webhcat",
	   "hive_log_dir" :"{{ LOG_DIR }}/hive"
        }
      },
     {
       "hive-site": {
            "javax.jdo.option.ConnectionPassword" : "{{HIVE_METASTORE_PASSWORD}}",
            "javax.jdo.option.ConnectionUserName" : "hive",
            "hive.server2.enable.doAs" : "false"
        }
        },
      {
      "hdfs-log4j" : {
           "content" : "\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\n# To change daemon root logger use hadoop_root_logger in hadoop-env\nhadoop.root.logger=INFO,console\nhadoop.log.dir=.\n\n\n\n# Define the root logger to the system property \"hadoop.root.logger\".\nlog4j.rootLogger=${hadoop.root.logger}, EventCounter,namenodeloggerId,secnamenodeloggerId,datanodeloggerId\n\n# Logging Threshold\nlog4j.threshhold=ALL\n\n#\n# Daily Rolling File Appender for namenode\n#\nlog4j.appender.namenodeloggerId=org.apache.log4j.DailyRollingFileAppender  \nlog4j.appender.namenodeloggerId.layout=org.apache.log4j.PatternLayout  \nlog4j.appender.namenodeloggerId.layout.=%d [%t] %-5p (%F:%L) - %m%n  \nlog4j.appender.namenodeloggerId.File=${hadoop.log.dir}/${hadoop.log.file}\nlog4j.appender.namenodeloggerId.DatePattern='-'yyyyMMdd-HH\n\n#\n# Daily Rolling File Appender for secnamenode\n#\nlog4j.appender.secnamenodeloggerId=org.apache.log4j.DailyRollingFileAppender  \nlog4j.appender.secnamenodeloggerId.layout=org.apache.log4j.PatternLayout  \nlog4j.appender.secnamenodeloggerId.layout.ConversionPattern=%d [%t] %-5p (%F:%L) - %m%n  \nlog4j.appender.secnamenodeloggerId.File=${hadoop.log.dir}/${hadoop.log.file}\nlog4j.appender.secnamenodeloggerId.DatePattern='-'yyyyMMdd-HH\n\n\n#\n# Daily Rolling File Appender for datanode\n#\nlog4j.appender.datanodeloggerId=org.apache.log4j.DailyRollingFileAppender  \nlog4j.appender.datanodeloggerId.layout=org.apache.log4j.PatternLayout  \nlog4j.appender.datanodeloggerId.layout.ConversionPattern=%d [%t] %-5p (%F:%L) - %m%n  \nlog4j.appender.datanodeloggerId.File=${hadoop.log.dir}/${hadoop.log.file}\nlog4j.appender.datanodeloggerId.DatePattern='-'yyyyMMdd-HH\n\n#\n# console\n# Add \"console\" to rootlogger above if you want to use this\n#\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n\n\n#\n# TaskLog Appender\n#\n\n#Default values\nhadoop.tasklog.taskid=null\nhadoop.tasklog.iscleanup=false\nhadoop.tasklog.noKeepSplits=4\nhadoop.tasklog.totalLogFileSize=100\nhadoop.tasklog.purgeLogSplits=true\nhadoop.tasklog.logsRetainHours=12\n\nlog4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender\nlog4j.appender.TLA.taskId=${hadoop.tasklog.taskid}\nlog4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup}\nlog4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize}\n\nlog4j.appender.TLA.layout=org.apache.log4j.PatternLayout\n#log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\nlog4j.appender.TLA.layout.ConversionPattern=%d [%t] %-5p (%F:%L) - %m%n\n\n#\n#Security audit appender\n#\nhadoop.security.logger=INFO,console\nhadoop.security.log.maxfilesize=256MB\nhadoop.security.log.maxbackupindex=20\nlog4j.category.SecurityLogger=${hadoop.security.logger}\nhadoop.security.log.file=SecurityAuth.audit\nlog4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}\nlog4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.DRFAS.layout.ConversionPattern=%d [%t] %-5p (%F:%L) - %m%n\nlog4j.appender.DRFAS.DatePattern=.yyyyMMdd-HH\n\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d [%t] %-5p (%F:%L) - %m%n\nlog4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}\n\n#\n# hdfs audit logging\n#\nhdfs.audit.logger=INFO,console\nlog4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}\nlog4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false\nlog4j.appender.DRFAAUDIT=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log\nlog4j.appender.DRFAAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.DRFAAUDIT.layout.ConversionPattern=%d [%t] %-5p (%F:%L) - %m%n\nlog4j.appender.DRFAAUDIT.DatePattern=.yyyyMMdd-HH\n\n#\n# mapred audit logging\n#\nmapred.audit.logger=INFO,console\nlog4j.logger.org.apache.hadoop.mapred.AuditLogger=${mapred.audit.logger}\nlog4j.additivity.org.apache.hadoop.mapred.AuditLogger=false\nlog4j.appender.MRAUDIT=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.MRAUDIT.File=${hadoop.log.dir}/mapred-audit.log\nlog4j.appender.MRAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.MRAUDIT.layout.ConversionPattern=%d [%t] %-5p (%F:%L) - %m%n\nlog4j.appender.MRAUDIT.DatePattern=.yyyyMMdd-HH\n\n\n# Custom Logging levels\n\nhadoop.metrics.log.level=INFO\n#log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG\n#log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.metrics2=${hadoop.metrics.log.level}\n\n# Jets3t library\nlog4j.logger.org.jets3t.service.impl.rest.httpclient.RestS3Service=ERROR\n\n#\n# Null Appender\n# Trap security logger on the hadoop client side\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n# Removes \"deprecated\" messages\nlog4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN\n\n#\n# HDFS block state change log from block manager\n#\n# Uncomment the following to suppress normal block state change\n# messages from BlockManager in NameNode.\n#log4j.logger.BlockStateChange=WARN"
        }
      },
      {
      "spark-log4j-properties" : {
	"content" : "\n# Set everything to be logged to the console\nlog4j.rootCategory=INFO, console,sparkloggerId\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\n# Daily Rolling File Appender for spark \nlog4j.appender.sparkloggerId=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.sparkloggerId.layout=org.apache.log4j.PatternLayout \nlog4j.appender.sparkloggerId.layout.ConversionPattern=%d [%t] %-5p (%F:%L) - %m%n \nlog4j.appender.sparkloggerId.File=${spark.log.dir}/spark.log\nlog4j.appender.sparkloggerId.DatePattern='-'yyyyMMdd-HH \n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n\n    "	
}
      },
      {
      "zookeeper-log4j" : {
	"content" : "\n#\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n # Daily Rolling File Appender for zookeeper \nlog4j.appender.zookeeperloggerId=org.apache.log4j.DailyRollingFileAppender \nlog4j.appender.zookeeperloggerId.layout=org.apache.log4j.PatternLayout \nlog4j.appender.zookeeperloggerId.layout.=%d [%t] %-5p (%F:%L) - %m%n \nlog4j.appender.zookeeperloggerId.File=${zookeeper.log.dir}/zookeeper.log \nlog4j.appender.zookeeperloggerId.DatePattern='-'yyyyMMdd-HH \n\n#\n#\n#\n\n#\n# ZooKeeper Logging Configuration\n#\n\n# DEFAULT: console appender only\nlog4j.rootLogger=INFO, CONSOLE,ROLLINGFILE\n\n# Example with rolling log file\n#log4j.rootLogger=DEBUG, CONSOLE, ROLLINGFILE\n\n# Example with rolling log file and tracing\n#log4j.rootLogger=TRACE, CONSOLE, ROLLINGFILE, TRACEFILE\n\n#\n# Log INFO level and above messages to the console\n#\nlog4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\nlog4j.appender.CONSOLE.Threshold=INFO\nlog4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.CONSOLE.layout.ConversionPattern=%d [%t] - %-5p [%t:%C{1}@%L] - %m%n\n\n#\n# Add ROLLINGFILE to rootLogger to get log file output\n#    Log DEBUG level and above messages to a log file\nlog4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender\nlog4j.appender.ROLLINGFILE.Threshold=DEBUG\nlog4j.appender.ROLLINGFILE.File=zookeeper.log\n\n# Max log file size of 10MB\nlog4j.appender.ROLLINGFILE.MaxFileSize=10MB\n# uncomment the next line to limit number of backup files\n#log4j.appender.ROLLINGFILE.MaxBackupIndex=10\n\nlog4j.appender.ROLLINGFILE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.ROLLINGFILE.layout.ConversionPattern=%d [%t] - %-5p [%t:%C{1}@%L] - %m%n\n\n\n#\n# Add TRACEFILE to rootLogger to get log file output\n#    Log DEBUG level and above messages to a log file\nlog4j.appender.TRACEFILE=org.apache.log4j.FileAppender\nlog4j.appender.TRACEFILE.Threshold=TRACE\nlog4j.appender.TRACEFILE.File=zookeeper_trace.log\n\nlog4j.appender.TRACEFILE.layout=org.apache.log4j.PatternLayout\n### Notice we are including log4j's NDC here (%x)\nlog4j.appender.TRACEFILE.layout.ConversionPattern=%d [%t] - %-5p [%t:%C{1}@%L][%x] - %m%n\n    "
        }

      },

      {
      "yarn-log4j" : {
        "content" : "\n#Relative to Yarn Log Dir Prefix\nyarn.log.dir=.\n#\n# Job Summary Appender\n#\n# Use following logger to send summary to separate file defined by\n# hadoop.mapreduce.jobsummary.log.file rolled daily:\n# hadoop.mapreduce.jobsummary.logger=INFO,JSA\n#\nhadoop.mapreduce.jobsummary.logger=${hadoop.root.logger},yarnloggerId,RMSUMMARY\nhadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log\nlog4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender\n\n# Daily Rolling File Appender for namenode\nlog4j.appender.yarnloggerId=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.yarnloggerId.layout=org.apache.log4j.PatternLayout \nlog4j.appender.yarnloggerId.layout.=%d [%t] %-5p (%F:%L) - %m%n \nlog4j.appender.yarnloggerId.File=${yarn.nodemanager.log-dirs}/${hadoop.log.file}    \nlog4j.appender.yarnloggerId.DatePattern='-'yyyyMMdd-HH\n\n# Set the ResourceManager summary log filename\nyarn.server.resourcemanager.appsummary.log.file=hadoop-mapreduce.jobsummary.log\n# Set the ResourceManager summary log level and appender\nyarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}\n#yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY\n\n# To enable AppSummaryLogging for the RM,\n# set yarn.server.resourcemanager.appsummary.logger to\n# LEVEL,RMSUMMARY in hadoop-env.sh\n\n# Appender for ResourceManager Application Summary Log\n# Requires the following properties to be set\n#    - hadoop.log.dir (Hadoop Log directory)\n#    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)\n#    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)\nlog4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender\nlog4j.appender.RMSUMMARY.File=${yarn.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}\nlog4j.appender.RMSUMMARY.MaxFileSize=256MB\nlog4j.appender.RMSUMMARY.MaxBackupIndex=20\nlog4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RMSUMMARY.layout.ConversionPattern=%d [%t] %p %c{2}: %m%n\nlog4j.appender.JSA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n\nlog4j.appender.JSA.DatePattern=.yyyy-MM-dd\nlog4j.appender.JSA.layout=org.apache.log4j.PatternLayout\nlog4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}\nlog4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false\n\n# Appender for viewing information for errors and warnings\nyarn.ewma.cleanupInterval=300\nyarn.ewma.messageAgeLimitSeconds=86400\nyarn.ewma.maxUniqueMessages=250\nlog4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender\nlog4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}\nlog4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}\nlog4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}\n    "
        }

      }



 ], 
  "host_groups" : [
    {
      "name" : "master",
    
      "components" : [
        {
          "name" : "NAMENODE"
        },
	{
          "name" : "APP_TIMELINE_SERVER"
        },
        {
          "name" : "RESOURCEMANAGER"
        },
        {
          "name" : "HISTORYSERVER"
        },
        {
          "name" : "SPARK_JOBHISTORYSERVER"
        },
	{
          "name" : "SECONDARY_NAMENODE"
        },
        {
          "name" : "DATANODE"
        },
        {
          "name" : "HDFS_CLIENT"
        },
        {
          "name" : "NODEMANAGER"
        },
        {
          "name" : "YARN_CLIENT"
        },
        {
          "name" : "PIG"
        },
        {
          "name" : "TEZ_CLIENT"
        },
        {
          "name" : "SPARK_CLIENT"
        },
	{
          "name" : "ZOOKEEPER_SERVER"
	},
        {
          "name" : "ZOOKEEPER_CLIENT"
	},
        {
          "name" : "HIVE_CLIENT"
        },
        {
          "name" : "HIVE_METASTORE"
        },
        {
          "name" : "HIVE_SERVER"
        },
        {
          "name" : "MYSQL_SERVER"
        },
        {
          "name" : "HCAT"
        },
        {
          "name" : "WEBHCAT_SERVER"
        },
        {
          "name" : "MAPREDUCE2_CLIENT"
        },
        {
          "name" : "HBASE_REGIONSERVER"
        },
        {
          "name" : "HBASE_MASTER"
        },
        {
          "name" : "HBASE_CLIENT"
        },
        {
          "name" : "OOZIE_CLIENT"
        },
        {
          "name" : "OOZIE_SERVER"
        },
        {
          "name" : "SQOOP"
        },
        {
          "name" : "FALCON_CLIENT"
        },
        {
          "name" : "FALCON_SERVER"
        },
        {
        "name" : "FLUME_HANDLER"
         },
        {
        "name" : "KNOX_GATEWAY"
         },
        {
        "name" : "SLIDER"
         },
        {
        "name" : "KAFKA_BROKER"
         }
      ],
      "cardinality" : "1+"
    }
  ],
  "Blueprints" : {
    "blueprint_name" : "blueprint",
    "stack_name" : "HDP",
    "stack_version" : "{{ HDP_VER }}"
  }
}
